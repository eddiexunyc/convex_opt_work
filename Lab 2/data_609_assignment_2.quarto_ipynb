{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"DATA 609 - Homework 2: Applications of Least Squares\"\n",
        "author: Eddie Xu\n",
        "format: html \n",
        "editor: source\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## Instructions\n",
        "\n",
        "Please submit a .qmd file along with a rendered pdf to the Brightspace page for this assignment. You may use whatever language you like within your qmd file,\n",
        "I recommend python, julia, or R. \n",
        "\n",
        "## Problem 1: Online Updating for Least Squares and Autoregressive Time Series Models\n",
        "\n",
        "Many applications of least squares (and other statistical methods) involve \\emph{streaming data}, in which data is collected over a time period and the statistical model is updated as new data arrives. If the quantity of data arriving is very large, it may be inefficient or even impossible to refit the entire model on the entire dataset. Instead, we use techniques (often referred to as \\emph{online learning} which take the current model as a starting point and update them to incorporate the new data. \n",
        "\n",
        "The structure of least squares problems makes them amenable to online updating (sometimes this is called \"recursive\" least squares). The structure of the problem is as follows, at time $t$ we receive a vector of observations $\\mathbf{a}_t$ and an observation of our target variable $b_t$.\n",
        "\n",
        "The full set of all observations and target variable data that we have received up to time $t$ is contained in the following matrix and vector:\n",
        "\n",
        "$$\n",
        "A_{(t)} = \\begin{bmatrix} \\cdots\\, \\mathbf{a}_1^T\\, \\cdots \\\\ \n",
        "\\cdots\\, \\mathbf{a}_2^T\\, \\cdots \\\\ \\vdots \\\\ \\cdots\\, \\mathbf{a}_t^T\\, \\cdots \\end{bmatrix},\\quad \n",
        "\\mathbf{b}_{(t)} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_t \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "which says that row $j$ of the matrix $A_{(t)}$ is the $j$th observation $\\mathbf{a}_j^T$, and the $j$th entry of $\\mathbf{b}_{(t)}$ is $b_j$.\n",
        "\n",
        "\n",
        "Here we assume that the vectors $\\mathbf{a}$ each contain $n$ observations, so that $A_{(t)}$ is a $t\\times n$ matrix and the vector $\\mathbf{b}_{(t)}$ is a $t$-vector.\n",
        "\n",
        "As long as $t>n$, i.e. the number of time observations is greater than the number of features/data points in each observation, we can fit a linear model predicting the target\n",
        "as a function of the features $\\mathbf{a}$ by solving the following system of equations:\n",
        "\n",
        "$$\n",
        "(A^T_{(t)}A_{(t)})\\mathbf{x}_{(t)} = A_{(t)}^T\\mathbf{b}_{(t)}\n",
        "$$\n",
        "\n",
        "As the length of the time series increases, the computational difficulty of solving this problem also increases. However, it is possible to re-use work done on the previous\n",
        "time step to avoid solving the full system at each step.\n",
        "\n",
        "This algorithm is based on the fact that the _Gram Matrix_ $A^T_{(t+1)}A_{(t+1)}$ can be calculated from the Gram matrix $A^T_{(t)}A_{(t)}$ from the previous time step.\n",
        "$$\n",
        "A^T_{(t+1)}A_{(t+1)} = A^T_{(t)}A_{(t)} + \\mathbf{a}_{t+1}\\mathbf{a}_{t+1}^T\n",
        "$$\n",
        "Similarly, the product $A^T_{(t+1)} \\mathbf{b}_{(t+1)}$ can also be updated from the\n",
        "value on the previous time step:\n",
        "$$\n",
        "A^T_{(t+1)}\\mathbf{b}_{(t+1)} = A^T_{(t)}\\mathbf{b}_{(t)} + b_{t+1}\\mathbf{a}_{t+1}\n",
        "$$\n",
        "\n",
        "We can write an efficient algorithm to compute the updated least squares solution as follows:\n",
        "\n",
        "- Step 1: Pick an initial time $t$ such that $A_{t}$ is square or tall so that the \n",
        "least squares problem can be solved (i.e. wait for enough data to have built up before your start) and then calculate the Gram matrix and the product $A^T_{t} \\mathbf{b}_t$\n",
        "$$\n",
        "G_{(t)} = A^T_{(t)}A_{(t)}, \\quad \\mathbf{h}_{(t)} = A^T_{t}\\mathbf{b}_t\n",
        "$$\n",
        "- Step 2: Find the least squares solution at time $t$ by solving the linear \n",
        "system:\n",
        "$$\n",
        "G_{(t)}\\mathbf{x}_{(t)} = \\mathbf{h}_{(t+1)}\n",
        "$$\n",
        "\n",
        "- Step 3: When the next data points $\\mathbf{a}_{t+1}$ and $b_{t+1}$, update\n",
        "$G_{(t+1)}$ and $\\mathbf{h}_{(t+1)}$:\n",
        "\n",
        "$$ G_{(t+1)} = G_{(t)} + \\mathbf{a}_{t+1}\\mathbf{a}^T_{t+1},\\quad \n",
        "\\mathbf{h}_{(t+1)} = \\mathbf{h}_{(t)} + b_{t+1}\\mathbf{a}_{t+1}\n",
        "$$\n",
        "Then you can repeat Step 2 to find $\\mathbf{x}_{t+1}$. This algorithm can be improved upon slightly using the Matrix Inversion Lemma/Woodbury Formula, which could be a topic for a project (see note at the end which mentions Kalman filters).\n",
        "\n",
        "(a) You are going to use this algorithm to make a linear, autoregressive model that predicts total day-ahead citibike trips from the daily high temperature and the number of daily citibike trips taken each of the past 7 days. The data is contained in the file [daily_citibike_trips.csv](https://github.com/georgehagstrom/DATA609Spring2025/blob/main/website/assignments/labs/labData/daily_citibike_trips.csv). \n",
        "\n",
        "Specifically, for each time point $t>7$ , fit the following\n",
        "model as a least squares estimation problem:\n",
        "$$\n",
        "N_{trips,\\tau} = \\sum_{i=1}^7 C_i N_{trips,\\tau-i} + C_T T,\n",
        "$$\n",
        "Here, each $N_{trips,\\tau}$ stands for the number of citibike trips on the $t$th day of the time series, $T$ stands for the forecast high temperature in New York City that day, and the coefficients $C_i$ and $C_T$ are the decision variables. \n",
        "\n",
        "Find the coefficients $C_{i,t}$ and $C_{T,t}$ that minimize the mean square errors on all the observed citibike trips prior to time $t$. Use the recursive least squares optimization outlined in the preamble to this problem to calculate the coefficients for each time point, and plot how they and the $R^2$ of the model change over time.\n",
        "\n",
        "What patterns do you notice in how the regression coefficients and $R^2$ change over time?\n",
        "\n",
        "Tip: Be very cautious when coding about the dimensionality of matrices and arrays.\n",
        "In python, `a @ a.T` will be an 8x8 matrix if `a.shape = (8,1)`. However, by default\n",
        "`a.shape = (8,)`, indicating that `a` is not being treated as either a row or column vector. For this problem it is important that the vectors are either row or column vectors, and not arrays without such an orientation. In python, you can use `numpy.reshape` to adjust. \n",
        "\n",
        "(b) We have included temperature as a variable because it probably influences the decision to ride a citibike. However, the relationship could be nonlinear, as both extreme high and low temperatures make bike riding less comfortable. With this idea in mind, create\n",
        "a new feature by choosing a nonlinear function of the temperature $T$ that represents\n",
        "the potential for both high and low values of $T$ to the same impact on predicted\n",
        "ridership. Use least squares optimization to fit an autoregressive time series model, replacing $T$ with the value of your \n",
        "new feature $f(T)$ in the time series. How does the temperature dependence coefficient differ between this model and the one you fit in (a)? Does the accuracy of the model improve or get worse using the new feature?\n"
      ],
      "id": "da419555"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# load dependencies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "## Part A\n",
        "# read in data\n",
        "citi_bike_raw_link = 'https://github.com/georgehagstrom/DATA609Spring2025/blob/main/website/assignments/labs/labData/daily_citibike_trips.csv'\n",
        "citi_bike_data = pd.read_csv(citi_bike_raw_link)\n"
      ],
      "id": "b338101f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 2: Weighted Least Squares\n",
        "\n",
        "The file [social-mobility.csv](https://github.com/georgehagstrom/DATA609Spring2025/blob/main/website/assignments/labs/labData/social_mobility.csv) contains data on the fraction of individuals born in the years 1980-1982 to parents in the bottom 20\\% of the income distribution who reach the top 20\\% of the income distribution by the time they turn 30 in a large number of municipalities throughout the United States. The dataset also contains additional variables that describe other socio-economic differences  between the cities in the dataset.\n",
        "\n",
        "(a) Make a scatter-plot of mobility versus population (use a log-scale for population). What do you notice about the variance of social mobility as a function of population? This is a common feature of nearly every dataset containing geographic regions with widely different populations.\n",
        "\n",
        "(b) Assume that the number of children born in families making below the 20th percentile of the income distribution in each city is linearly proportional to the city population. Write down a formula for how the variance of each measurement of the social mobility should depend on the measured social mobility and the population. Hint: start with either the formula for the variance of binomial counts or look up the variance of a proportion derived from a binomial distribution. Don't worry about constant factors when deriving this formula.\n",
        "\n",
        "(c) Use weighted least squares to calculate an estimate of how social mobility depends on commute time and student-teacher ratio, using weights calculated based on the variance estimate derived in (b).\n",
        "Compare the coefficients to those derived from ordinary least squares with no weights.\n",
        "\n",
        "## Problem 3: Markowitz Portfolio Optimization\n",
        "\n",
        "In this problem you will use _Markowitz Portfolio Optimization_ to construct a set of portfolios that aim to achieve target expected rates of return while minimizing risk. The file [stock_returns.csv](https://github.com/georgehagstrom/DATA609Spring2025/blob/main/website/assignments/labs/labData/stock_returns.csv) contains information on daily asset returns from 2020-2024 for a group of assets, consisting mostly of large-cap stocks but also a handful of exchange traded funds that correspond to US Treasury Bonds and Notes with varying maturities.\n",
        "\n",
        "You will divide the data into two time periods, a training period (2020-2022) and a testing period (2022-2024). The data in the `stock_returns.csv` is stored in a long format, with the \n",
        "following variables:\n",
        "\n",
        "  1. `Company` - The ticker symbol that identifies the stock\n",
        "  2. `date` - The date to which the data corresponds\n",
        "  3. `adjusted`- the closing price of the stock, adjusted for special events like   dividends and stock splits\n",
        "  4. `return` - the ratio of the current adjusted close to the adjusted close on the previous trading day\n",
        "  5. `log_return`- the natural logarithm of the return\n",
        "\n",
        "\n",
        "(a)  Construct a vector (we call it $\\mu$) containing the annualized rate of return over the training period (for the $i$th stock, you can use the formula: $\\mu_i = \\exp\\left(0.5\\sum_{t} \\mathrm{log\\_return}_i(t)\\right)$), or the square root of the total return over the first two years of data,  and daily return covariance\n",
        "$\\Gamma$. \n",
        "\n",
        "__Hint__: Possible workflow if  working in python: convert your data to a wide format using `pandas`, extract the values into a `numpy ndarray`, and then use the function `np.cov`. \n",
        "\n",
        "Then solve the following constrained least squares problem to calculate optimal portfolios achieving a fixed rate of return with minimum variance:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "    \\min_{w} \\mathbf{x}^T\\Gamma \\mathbf{x}, \\\\\n",
        "    \\mathbf{w}^T\\mathbf{\\mu} = r, \\\\\n",
        "    \\sum_{i} w_i = 1\n",
        "\\end{aligned}\n",
        "$$\n",
        "Here $\\mathbf{w}$ is a vector containing the investment allocations into different\n",
        "assets, and $r$ is the target rate of return.\n",
        "\n",
        "Calculate optimal portfolios based on the 2020-2022 data for $r=1.05$, $r=1.10$, and $r=1.20$.\n",
        "\n",
        "(b) Plot the cumulative value of each portfolio over time, assuming that an \n",
        "initial investment is made at the start of the period and that there is no rebalancing of the portfolio, i.e. $r_{T} = \\sum_{i=1}^n w_i\\Pi_{t=1}^T r_{it}$, where $r_{it}$ is the return of asset $i$ on trading day $t$, (hint: `np.cumprod` allows you to efficiently calculate this quantity).\n",
        "Make the plot for both the training and test sets of returns.\n",
        "\n",
        "For each of the 3 portfolios also report:\n",
        "\n",
        "- The annualized return on the training and test sets;\n",
        "- The risk on the training and test sets, defined as the \n",
        "realized variance of the daily return;\n",
        "- The asset with the maximum allocation weight, and its weight;\n",
        "- The initial leverage, defined as $\\sum_{i=1}^n |w_i|$. This\n",
        "number is always at least one, and it is exactly one only if the portfolio has no short positions.\n",
        "\n",
        "Comment briefly on your observations about the different portfolios and the difference between their training and testing performance.\n",
        "\n",
        "(c) It is well known that optimal portfolios constructed using the\n",
        "Markowitz procedure perform much more poorly out of sample compared to in sample. This is due to a variety of reasons, one of which is that the procedure assumes that future returns are equal to past returns, another that\n",
        "the correlation structure of the market might change over time, and\n",
        "finally, when there are many assets there is the potential for overfitting. Repeat the previous problem but introduce a ridge regression/$l_2$ norm penalty term to the objective function, with a hyperparameter $\\lambda$ governing the size of the penalty term. \n",
        "\n",
        "Specifically, you will select 10 positive values of $\\lambda$ on a log scale between $1e-1$ and $10$ and for each value of $\\lambda$ solve the following penalized\n",
        "regression problem:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "    \\min_{w} w^T(\\Gamma+\\lambda I) w ,\\\\\n",
        "    w^T\\mu = r,\\\\\n",
        "    \\sum_{i=1}^n w_i = 1\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "for just the single value of $r=20\\%$. Then calculate the performance\n",
        "of each of these regularized Markowitz strategies on both the training and test datasets\n",
        "and plot the the return of the portfolios over both the training and testing period.\n",
        "\n",
        "For each portfolio, also report:\n",
        "\n",
        "- The annualized return on the training and test sets;\n",
        "- The risk on the training and test sets;\n",
        "- The maximum allocation weight and the asset with maximum allocation;\n",
        "- The initial leverage\n",
        "\n",
        "Comment on how the different values of $\\lambda$ changed the optimal\n",
        "portfolios and the difference between in-sample and out-of-sample return and variance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Potential Projects Based on this Homework:\n",
        "\n",
        "Recursive least squares is a gateway to several important\n",
        "techniques that would lead to good projects. The Kalman Filter or Bayes Filter is an algorithm that recursively estimates a statistical model while allowing the underlying coefficients (or state) of that model to undergo dynamical evolution (as a simple example, think of correcting noisy measurements of the GPS location of a drone using Newtownian physics). These are some of the most useful prediction algorithms and can be applied to many areas, including econometrics, web traffic prediction, and vehicle location. For a more math related project, studying the accuracy of the Woodbury formula could be interesting. "
      ],
      "id": "23bdf778"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/eddiexuexia/opt/anaconda3/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}