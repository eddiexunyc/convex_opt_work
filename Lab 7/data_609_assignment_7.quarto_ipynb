{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"DATA 609 - Homework 7: Nonconvex Optimization and Deep Learning\"\n",
        "format: html\n",
        "editor: source\n",
        "author: Eddie Xu\n",
        "---\n",
        "\n",
        "\n",
        "## Instructions\n",
        "\n",
        "For this problem I recommend submitting a python notebook or a quarto file. It might be easier with a python notebook. You can use google colab if you do not have enough computational power in your personal computer.\n",
        "\n",
        "## Problem 1: Comparing Optimization Algorithms\n",
        "\n",
        "Consider the function, $f(x,y) = (1-x^2) + 100(y - x^2)^2)$ which has a global minimum at, $x = 1$. For this problem, you are going to explore using different optimization algorithms to find the global minimum, to gain an intuitive understanding of their different strengths and weaknesses.\n",
        "\n",
        "(a) Make a contour plot of this function. You should observe a that the contour lines are “banana-shaped” around the global minimum point, which lies in a deep valley. In technical terms, we would say that the gradient of this function is strongly anisotropic, a fact that can cause slow or no convergence for optimization algorithms.\n",
        "\n",
        "### Problem 1(a) Solution\n"
      ],
      "id": "cc13760a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# load dependencies\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# define function\n",
        "def prob_func(x, y):\n",
        "    return (1 - x)**2 + 100 * (y - x**2)**2\n",
        "\n",
        "# Create a grid of (x, y) values\n",
        "x = np.linspace(-4, 4, 400)\n",
        "y = np.linspace(-2, 6, 400)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "Z = prob_func(X, Y)\n",
        "\n",
        "# Plot the contour\n",
        "plt.figure(figsize=(16, 12))\n",
        "contours = plt.contour(X, Y, Z, levels=np.logspace(-1, 3.5, 20), cmap='viridis')\n",
        "plt.clabel(contours, inline=True, fontsize=8)\n",
        "plt.plot(1, 1, 'r*', markersize=10, label='Global Minimum (1, 1)')\n",
        "plt.title('Contour Plot of the function')\n",
        "plt.legend(True)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "id": "e51f5f7c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(b) In the code chunk below I have python code for three different optimization algorithms,(1) stochastic gradient descent; (2) stochastic gradient descent with momentum, and (3) ADAM (ADAptive Moment Estimation). Starting at the initial point $x= -4, y = -2$, use each algorithm to find the minimum of the function $f$. Start with a learning rate of $\\kappa = 10^{-4}$ for all three algorithms, and run the algorithm for $10^5$ timesteps. Plot the trajectories of each algorithm and the log base 10 of the error rate as a function of the time step. What do you notice about the performance of the difference algorithms, both in terms of convergence speed and ultimate accuracy?\n"
      ],
      "id": "76cea1d2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# available defined function\n",
        "def gd(grad, init, n_epochs=1000, eta=10**-4):\n",
        "\n",
        "    params=np.array(init)\n",
        "    param_traj=np.zeros([n_epochs+1,2])\n",
        "    param_traj[0,]=init\n",
        "    v=0;\n",
        "    for j in range(n_epochs):\n",
        "        v=eta*(np.array(grad(params)))\n",
        "        params=params-v\n",
        "        param_traj[j+1,]=params\n",
        "    return param_traj\n",
        "\n",
        "\n",
        "def gd_with_mom(grad, init, n_epochs=5000, eta=10**-4, beta=0.9,gamma=0.9):\n",
        "    params=np.array(init) # Start with initial condition\n",
        "    param_traj=np.zeros([n_epochs+1,2]) # Save the entire trajecotry\n",
        "    param_traj[0,]=init # Also save the initial condition to the trajectory\n",
        "    \n",
        "    v=0 # Starting with 0 momentum\n",
        "    \n",
        "    # Epochs is borrowing term from machine learning\n",
        "    # Here it means timestep\n",
        "    \n",
        "    for j in range(n_epochs): \n",
        "        v=gamma*v+(np.array(grad(params))) # Compute v\n",
        "        params=params-eta*v  # Update the location\n",
        "        param_traj[j+1,]=params # Save the trajectory\n",
        "    return param_traj\n",
        "\n",
        "def adams(grad, init, n_epochs=5000, eta=10**-4, gamma=0.9, beta=0.99,epsilon=10**-8):\n",
        "    params=np.array(init)\n",
        "    param_traj=np.zeros([n_epochs+1,2])\n",
        "    param_traj[0,]=init\n",
        "    v=0;\n",
        "    grad_sq=0;\n",
        "    for j in range(n_epochs):\n",
        "        g=np.array(grad(params))\n",
        "        v=gamma*v+(1-gamma)*g\n",
        "        grad_sq=beta*grad_sq+(1-beta)*g*g\n",
        "        v_hat=v/(1-gamma**(j+1))\n",
        "        grad_sq_hat=grad_sq/(1-beta**(j+1))\n",
        "        params=params-eta*np.divide(v_hat,np.sqrt(grad_sq_hat+epsilon))\n",
        "        param_traj[j+1,]=params\n",
        "    return param_traj\n",
        "\n",
        "# load dependencies\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# define function\n",
        "def prob_func(x, y):\n",
        "    return (1 - x)**2 + 100 * (y - x**2)**2\n",
        "\n",
        "def prob_func_grad(xy):\n",
        "    x, y = xy\n",
        "    dfdx = -2 * (1 - x) - 400 * x * (y - x**2)\n",
        "    dfdy = 200 * (y - x**2)\n",
        "    return np.array([dfdx, dfdy])\n",
        "\n",
        "# intitalize the optimizer\n",
        "init_point = [-4.0, -2.0]\n",
        "eta = 1e-4\n",
        "n_epochs = 100000\n",
        "\n",
        "# run the optimizer\n",
        "gd_traj = gd(prob_func_grad, init_point, n_epochs=n_epochs, eta=eta)\n",
        "mom_traj = gd_with_mom(prob_func_grad, init_point, n_epochs=n_epochs, eta=eta)\n",
        "adam_traj = adams(prob_func_grad, init_point, n_epochs=n_epochs, eta=eta)\n",
        "\n",
        "# compute log10 error\n",
        "true_min = np.array([1, 1])\n",
        "gd_error = np.log10(np.sum((gd_traj - true_min)**2, axis=1))\n",
        "mom_error = np.log10(np.sum((mom_traj - true_min)**2, axis=1))\n",
        "adam_error = np.log10(np.sum((adam_traj - true_min)**2, axis=1))\n",
        "\n",
        "# create a contour plot of the function\n",
        "x = np.linspace(-4.5, 4.5, 400)\n",
        "y = np.linspace(-3, 3, 400)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "Z = prob_func(X, Y)\n",
        "\n",
        "# plot results\n",
        "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# plot the contour with trajectories\n",
        "axs[0].contour(X, Y, Z, levels=np.logspace(-1, 3.5, 20), cmap='viridis')\n",
        "axs[0].plot(gd_traj[:, 0], gd_traj[:, 1], label='Gradient Descent')\n",
        "axs[0].plot(mom_traj[:, 0], mom_traj[:, 1], label='Momentum')\n",
        "axs[0].plot(adam_traj[:, 0], adam_traj[:, 1], label='Adam')\n",
        "axs[0].plot(1, 1, 'r*', markersize=10)\n",
        "axs[0].set_title(\"Trajectories on Contour Plot (η = 1e-4)\")\n",
        "axs[0].legend()\n",
        "\n",
        "# plot the log error\n",
        "axs[1].plot(gd_error, label='Gradient Descent')\n",
        "axs[1].plot(mom_error, label='Momentum')\n",
        "axs[1].plot(adam_error, label='Adam')\n",
        "axs[1].set_title(\"Log10 of Squared Error vs Timestep (η = 1e-4)\")\n",
        "axs[1].set_xlabel(\"Timestep\")\n",
        "axs[1].set_ylabel(\"log10(Error)\")\n",
        "axs[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "1ed5f085",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(c) Perform the same experiment for the learning rate $\\kappa = 10^{-3}$, only comparing ADAM and gradient descent with momentum. You will likely observe that one of the methods does not converge, keep the same range of values for your trajectory/contour plot as you did in (b). Which method worked better with $\\kappa = 10^{3}$?\n",
        "\n",
        "(d) Now perform a comparison between ADAM with $\\kappa=10^{-2}$ against gradient descent with momentum using . What are the trade-offs between the two methods for these values of the learning rate?\n",
        "\n",
        "### Problem 1(c) and 1(d) Solution\n"
      ],
      "id": "f6112a1b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# load dependencies\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# define function\n",
        "def prob_func(x, y):\n",
        "    return (1 - x)**2 + 100 * (y - x**2)**2\n",
        "\n",
        "def prob_func_grad(xy):\n",
        "    x, y = xy\n",
        "    dfdx = -2 * (1 - x) - 400 * x * (y - x**2)\n",
        "    dfdy = 200 * (y - x**2)\n",
        "    return np.array([dfdx, dfdy])\n",
        "\n",
        "# set the applied the function\n",
        "def adams(grad, init, n_epochs=5000, eta=10**-4, gamma=0.9, beta=0.99,epsilon=10**-8):\n",
        "    params=np.array(init)\n",
        "    param_traj=np.zeros([n_epochs+1,2])\n",
        "    param_traj[0,]=init\n",
        "    v=0;\n",
        "    grad_sq=0;\n",
        "    for j in range(n_epochs):\n",
        "        g=np.array(grad(params))\n",
        "        v=gamma*v+(1-gamma)*g\n",
        "        grad_sq=beta*grad_sq+(1-beta)*g*g\n",
        "        v_hat=v/(1-gamma**(j+1))\n",
        "        grad_sq_hat=grad_sq/(1-beta**(j+1))\n",
        "        params=params-eta*np.divide(v_hat,np.sqrt(grad_sq_hat+epsilon))\n",
        "        param_traj[j+1,]=params\n",
        "    return param_traj\n",
        "\n",
        "def gd_with_mom(grad, init, n_epochs=5000, eta=10**-4, beta=0.9,gamma=0.9):\n",
        "    params=np.array(init) # Start with initial condition\n",
        "    param_traj=np.zeros([n_epochs+1,2]) # Save the entire trajecotry\n",
        "    param_traj[0,]=init # Also save the initial condition to the trajectory\n",
        "    \n",
        "    v=0 # Starting with 0 momentum\n",
        "    \n",
        "    # Epochs is borrowing term from machine learning\n",
        "    # Here it means timestep\n",
        "    \n",
        "    for j in range(n_epochs): \n",
        "        v=gamma*v+(np.array(grad(params))) # Compute v\n",
        "        params=params-eta*v  # Update the location\n",
        "        param_traj[j+1,]=params # Save the trajectory\n",
        "    return param_traj\n",
        "\n",
        "# intitalize the optimizer\n",
        "init_point = [-4.0, -2.0]\n",
        "n_epochs = 100000\n",
        "\n",
        "# define the eta for part c\n",
        "eta_c = 1e-3\n",
        "\n",
        "# run the optimizer for the momentum and ADAM with higher learning rate\n",
        "mom_traj_c = gd_with_mom(prob_func_grad, init_point, n_epochs=n_epochs, eta=eta_c)\n",
        "adam_traj_c = adams(prob_func_grad, init_point, n_epochs=n_epochs, eta=eta_c)\n",
        "\n",
        "# compute the log 10 error\n",
        "true_min = np.array([1, 1])\n",
        "mom_error_c = np.log10(np.sum((mom_traj_c - true_min)**2, axis=1))\n",
        "adam_error_c = np.log10(np.sum((adam_traj_c - true_min)**2, axis=1))\n",
        "\n",
        "# plot\n",
        "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# plot the contour with trajectories\n",
        "axs[0].contour(X, Y, Z, levels=np.logspace(-1, 3.5, 20), cmap='viridis')\n",
        "axs[0].plot(mom_traj_c[:, 0], mom_traj_c[:, 1], label='Momentum (η=1e-3)')\n",
        "axs[0].plot(adam_traj_c[:, 0], adam_traj_c[:, 1], label='Adam (η=1e-3)')\n",
        "axs[0].plot(1, 1, 'r*', markersize=10)\n",
        "axs[0].set_title(\"Trajectories at η = 1e-3\")\n",
        "axs[0].legend()\n",
        "\n",
        "# plot the log error\n",
        "axs[1].plot(mom_error_c, label='Momentum (η=1e-3)')\n",
        "axs[1].plot(adam_error_c, label='Adam (η=1e-3)')\n",
        "axs[1].set_title(\"Log10 Error vs Time Step (η = 1e-3)\")\n",
        "axs[1].set_xlabel(\"Timestep\")\n",
        "axs[1].set_ylabel(\"log10(Error)\")\n",
        "axs[1].legend()\n",
        "\n",
        "# define the eta for part d\n",
        "eta_d = 1e-2\n",
        "\n",
        "# Run with even higher learning rate\n",
        "mom_traj_d = gd_with_mom(prob_func_grad, init_point, n_epochs=n_epochs, eta=eta_d)\n",
        "adam_traj_d = adams(prob_func_grad, init_point, n_epochs=n_epochs, eta=eta_d)\n",
        "\n",
        "# calculate errors\n",
        "mom_error_d = np.log10(np.sum((mom_traj_d - true_min)**2, axis=1))\n",
        "adam_error_d = np.log10(np.sum((adam_traj_d - true_min)**2, axis=1))\n",
        "\n",
        "# plot\n",
        "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# plot the contour with trajectories\n",
        "axs[0].contour(X, Y, Z, levels=np.logspace(-1, 3.5, 20), cmap='viridis')\n",
        "axs[0].plot(mom_traj_d[:, 0], mom_traj_d[:, 1], label='Momentum (η=1e-2)')\n",
        "axs[0].plot(adam_traj_d[:, 0], adam_traj_d[:, 1], label='Adam (η=1e-2)')\n",
        "axs[0].plot(1, 1, 'r*', markersize=10)\n",
        "axs[0].set_title(\"Trajectories at η = 1e-2\")\n",
        "axs[0].legend()\n",
        "\n",
        "\n",
        "# plot the log error\n",
        "axs[1].plot(mom_error_d, label='Momentum (η=1e-2)')\n",
        "axs[1].plot(adam_error_d, label='Adam (η=1e-2)')\n",
        "axs[1].set_title(\"Log10 Error vs Time Step (η = 1e-2)\")\n",
        "axs[1].set_xlabel(\"Timestep\")\n",
        "axs[1].set_ylabel(\"log10(Error)\")\n",
        "axs[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "485c18f7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 2: Shallow Nets and MNIST\n",
        "\n",
        "For this exercise, we will work on one of the standard model problems in Machine Learning, classifying handwritten digits. We will use an adaptation of the neural network code from your reading assignment to pytorch, which is one of the leading frameworks for training neural networks. pytorch is fairly flexible, you can use it with the CPU on your personal computer, with GPUs, and even on computing clusters. If you have trouble getting pytorch to work on your own computer I recommend trying in on google colab, or alternatively you are welcome to develop your own implementation. This and the next assignment have helper code in the provided ipython notebook Lab 7 Helper Noteook\n",
        "\n",
        "First, you should acquire the MNIST dataset. This can be downloaded automatically using pytorch via the following code chunk:\n"
      ],
      "id": "39331e46"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Here is some code that automatically downlaods the MNIST data. Technically it will also read the\n",
        "# data in if you have already downloaded and the path points to the folder where you have the files\n",
        "# There will be 4 binary files which together contain the testing and training examples and the labels\n",
        "# for the testing and training examples. \n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Load MNIST\n",
        "\n",
        "\n",
        "# transform defines a function which takes an image file, converts the analog bits into floating point numbers (it's a literal image file in the data), and then flattens the file. Each image is 28x28, so at the end we get a 784x1 vector\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
        "\n",
        "# The first line downloads the entire MNIST dataset to the data directory (or whereever you want it)\n",
        "# If the data is already there, this won't download it. THis downloads both the training and testing data.\n",
        "# the transform keyword applies the transform defined above, the train dataset has 60,000 examples, and\n",
        "# the test dataset has 10,000 examples. The train and test data is loaded in the variables.\n",
        "\n",
        "train_dataset = datasets.MNIST('data/', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('data/', train=False, transform=transform)"
      ],
      "id": "41d120fb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are going to train a simple neural network to classify the MNIST images. The neural network has an input layer of 784 neurons (one for each pixel), a 30 neuron hidden layer, and a 10 neuron output layer, which provided a weight that corresponds to the predictions of the neural network for each class. Initially we will use sigmoidal neurons to process the inputs. Throughout the rest of the assignment you will use and improve the code in this file to study the performance of different combinations of network structure, optimization algorithm choice, hyperparameters, and activation functions.\n",
        "\n",
        "The initial configuration of the neural network uses stochastic gradient descent without momentum. The following code sets several key hyperparameters and trains the neural network:\n"
      ],
      "id": "72aec1aa"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# batch_size determines the minibatch size\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)\n",
        "\n",
        "# Initialize network\n",
        "# net = Network([784, 30, 10])\n",
        "\n",
        "# Train\n",
        "# sol = train(net, train_loader, epochs=30, eta=0.001, test_data=test_loader)"
      ],
      "id": "e7661511",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Additional parameters can be changed within the code of the function itself (and you can organize your code in any way you see fit), in the train function:\n"
      ],
      "id": "825333cd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "    # optimizer = optim.SGD(network.parameters(),momentum=0.8,nesterov=True, lr=eta,weight_decay=1e-5)\n",
        "    \n",
        "    # This uncomment this (and comment the above line) if you want to use ADAM. The betas are the memory\n",
        "    # parameters, you can experiment with these hyperparaeters if you like:\n",
        "    #optimizer = optim.Adam(network.parameters(),betas = (0.9,0.999), lr=eta,weight_decay=1e-5)\n",
        "\n",
        "    # Here is code for using learning rate scheduling. You might find this helpful\n",
        "    #step_size = 2\n",
        "    #gamma = 0.7\n",
        "    #scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)"
      ],
      "id": "19d26ba1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and also in the class function:\n"
      ],
      "id": "ce140412"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self, sizes):\n",
        "        super(Network, self).__init__()\n",
        "        self.sizes = sizes\n",
        "        self.num_layers = len(sizes)\n",
        "        \n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(self.num_layers - 1):\n",
        "            layer = nn.Linear(sizes[i], sizes[i+1])\n",
        "            nn.init.xavier_normal_(layer.weight)   # Good initialization for shallow/sigmoid nets\n",
        "            #nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu') initialization for relus\n",
        "            #nn.init.kaiming_uniform_(layer.weight, mode='fan_out', nonlinearity='relu') initialization for relus and deep nets\n",
        "\n",
        "            nn.init.zeros_(layer.bias)               # initialize the bias to 0\n",
        "            self.layers.append(layer)\n",
        "    \n",
        "    # Forward is the method that calculates the value of the neural network. Basically we recursively apply the activations in each\n",
        "    # layer\n",
        "    \n",
        "    def forward(self, x):\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = F.sigmoid(layer(x))   # sigmoid layers\n",
        "            # x = F.relu(layer(x)) # You will try the relu layer in the last problem\n",
        "        x = self.layers[-1](x) \n",
        "        return x"
      ],
      "id": "d45f6b3a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The default set of hyperparameters lead to a neural network that successfully trains, and has an ultimate out of sample accuracy of just below 0.95 after 30 epochs of training on my computer.\n",
        "\n",
        "Validate this result by executing the train command with the parameters described above. Next, the learning rate is one of the most important hyperparameters in machine learning. Train the same structure of neural network with a range of different learning rates both higher and lower. Make sure you find a learning rate high enough so that the neural network performance is poor. What was the learning rate that led to the best accuracy at the end of 30 epochs? Plot the test accuracy as a function of epochs (it is one of the outputs of the train function) for all of the learning rates that you tested.\n",
        "\n",
        "How does the ADAM optimizer handle this problem? Modify the neural network code so that the optimizer is ADAM optimizer (you will see commented code in the train function). Then train a neural network using ADAM again with a range of different learning rates (including the same starting point  as in part(a)). Compare the behavior of the learning curves, the final accuracy, and the values of the learning rates that were most successful with part (a).\n",
        "\n",
        "How good can you make your 3-layer network? You don’t need to do an exhaustive search of all possible options (which would take forever) but experiment with the optimization algorithm, the learning rate and the other hyper-parameters. For example, you could include more or fewer neurons in the hidden layer, change the values of the betas in ADAM or the momentum in SGD, alter the batch size in the data loader, change the learning rate, increase or decrease the weight decay (which is L2 regularization), or change the number of training epochs.\n",
        "\n",
        "Some tips: - For this problem I don’t think it is likely to have an accuracy above 0.99 without expanding the data - Larger batch sizes have less “noise” so might need more regularization. They also sometimes require larger learning rates - The learning rate scheduler will decrease the learning rate by a factor of $gamma\\$ every steps number of epochs\n",
        "\n",
        "## Problem 3: Deep Nets: Overcoming Gradients\n",
        "\n",
        "In deep networks, the gradients for the neural network weights can either vanish or explode due to the compositional nature of the network. This typically happens for weights near the input layer. In order to observe vanishing gradients we need to calculate the size of the gradients of the loss function with respect to each weight. The neural network training routine has commented code that computes the norm of the gradients of the input layer and the norm of the gradients of the output layer and divides and averages them across all the batches in an epoch. Uncomment the code (and modify the print and return statement) so that the gradients are computed and the gradient ratio output and saved.\n",
        "Then try training a deep neural network. I recommend beginning with the architecture net = Network([784,30,30,30,30,30,30,30,30,10]). Train this neural network. You will almost certainly find that the training does not succeed. What are the gradient ratios that you observe during the training?\n",
        "\n",
        "If your computer cannot train this network, you can try this problem with a shallower network, or my recommendation is to use only a single epoch of training which should still demonstrate the vanishing gradient problem.\n",
        "\n",
        "There are several techniques to deal with vanishing (or exploding) gradients. These include using neurons with different activation functions or using different normalization schemes. The forward method in the Network class defines the activation functions. Change the activation function from sigmoid to relu and change the initialization from Xavier.normal_ to kaiming.uniform_ (which is more optimal for deep ReLU neurons) and train the deep neural network. How does the gradient ratio change? How does the test accuracy compare to the shallow net accuracy you achieved in problem 2?\n",
        "\n",
        "Deep neural networks are a superior architecture of image classification problems than shallow networks, however typically the dense structure that we have implemented here is not used. Instead the neural networks usually have several convolutionary layers at the beginning. With some effort and experimentation, it should still be possible to achieve a very high accuracy with a dense neural network. Experiment with the network architecture and the hyperparameters and see how good you can make your deep/dense network. You can try a combination of increasing the number of neurons in the hidden layers or incrasing the depth of the network. A structure that often works is one which decreases the number of neurons per hidden layer steadily from the input layer to the final layer (i.e. starting at 784 and ending at 10). Can you improve upon the best shallow network that you constructed for problem 3?"
      ],
      "id": "20e25486"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/eddiexuexia/opt/anaconda3/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}